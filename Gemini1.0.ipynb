{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RbSO1J9owbg0cTrOwjJiLRpK",
      "metadata": {
        "id": "RbSO1J9owbg0cTrOwjJiLRpK",
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aAdKWDJgS6k",
      "metadata": {
        "id": "4aAdKWDJgS6k"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform\n",
        "! pip3 install --upgrade --quiet google-cloud-aiplatform[autologging]\n",
        "\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "\n",
        "# Only if your bucket doesn't already exist: Uncomment the following code to create your Cloud Storage bucket.\n",
        "# ! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ie4dLjm9gS6l",
      "metadata": {
        "id": "ie4dLjm9gS6l"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "aiplatform.init(\n",
        "    project=PROJECT_ID,\n",
        "    staging_bucket=BUCKET_URI,\n",
        "    location=REGION,\n",
        "    experiment=EXPERIMENT_NAME)\n",
        "\n",
        "aiplatform.autolog()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H3VUbbydiENY",
      "metadata": {
        "id": "H3VUbbydiENY"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, FinishReason\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, ChatSession\n",
        "vertexai.init(project=\"praxis-granite-420515\", location=\"us-central1\")\n",
        "model=GenerativeModel(model_name=\"gemini-1.0-pro\",system_instruction=\" You are an intelligent programmer. You must complete the python function given to you by the user. And you must follow the format they present when giving your answer!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DzXcclR9jX33",
      "metadata": {
        "id": "DzXcclR9jX33"
      },
      "outputs": [],
      "source": [
        "!conda create -n codex python=3.7\n",
        "! conda activate codex\n",
        "! git clone https://github.com/openai/human-eval\n",
        "! pip install -e human-eval\n",
        "# Set the working directory\n",
        "%cd /content/human-eval\n",
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ToKS92yOSwq",
      "metadata": {
        "id": "6ToKS92yOSwq"
      },
      "outputs": [],
      "source": [
        "generation_config = {\n",
        "    \"temperature\": 0.0,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Iin5A4hchKFr",
      "metadata": {
        "id": "Iin5A4hchKFr"
      },
      "outputs": [],
      "source": [
        "from human_eval.data import write_jsonl, read_problems\n",
        "import pandas as pd\n",
        "import time\n",
        "import jsonlines\n",
        "def generate_one_completion(prompt):\n",
        "  prompt_msg =  \"You are an intelligent programmer. You must complete the python function given to you by the user. No explanation of function required And you must follow the format they present when giving your answer!Exclude examples\"+prompt\n",
        "  response = model.generate_content(prompt_msg,generation_config=generation_config)\n",
        "  return response.candidates[0].text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EnuSFOQC7VQW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3963214,
          "status": "ok",
          "timestamp": 1713392815854,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 240
        },
        "id": "EnuSFOQC7VQW",
        "outputId": "b4ab4903-25d5-41c5-bceb-1603c8cbfacc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.478311538696289\n",
            "1.478311538696289\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    jsonObj = pd.read_json(path_or_buf=\"https://github.com/openai/human-eval/raw/master/data/HumanEval.jsonl.gz\", lines=True)\n",
        "    command = 0\n",
        "    i = 0\n",
        "    durationTimes = []\n",
        "    with jsonlines.open('Gemini1.0-pro.jsonl', mode='w') as writer:\n",
        "        while i < 164:\n",
        "            if command < 4:\n",
        "                start_time = time.time()\n",
        "                response = generate_one_completion(jsonObj[\"prompt\"][i])\n",
        "                duration = time.time() - start_time\n",
        "\n",
        "                durationTimes.append(duration)\n",
        "                response = response.replace(\"```python\\n\", \"\")\n",
        "                response = response.replace(\"\\n```\", \"\")\n",
        "\n",
        "                completion = {'task_id': jsonObj[\"task_id\"][i], 'completion': response}\n",
        "                writer.write(completion)\n",
        "                command = command + 1\n",
        "                i = i + 1\n",
        "            else:\n",
        "                command = 0\n",
        "                time.sleep(90)\n",
        "    print(np.sum(duration))\n",
        "    print(np.average(duration))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "vnDF4OaK7dI5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 9216,
          "status": "ok",
          "timestamp": 1713393138039,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 240
        },
        "id": "vnDF4OaK7dI5",
        "outputId": "7b7712e0-ed7d-4352-9158-6f5b5eaf3a32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading samples...\n",
            "\r0it [00:00, ?it/s]\r164it [00:00, 14632.95it/s]\n",
            "Running test suites...\n",
            "100% 164/164 [00:03<00:00, 49.40it/s]\n",
            "Writing results to Gemini1.0-pro.jsonl_results.jsonl...\n",
            "100% 164/164 [00:00<00:00, 26120.83it/s]\n",
            "{'pass@1': 0.4146341463414634}\n"
          ]
        }
      ],
      "source": [
        "!python3 human_eval/evaluate_functional_correctness.py Gemini1.0-pro.jsonl"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "unnatichaturvedi295 (Apr 16, 2024, 11:22:56â€¯AM)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
